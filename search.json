[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/13주차과제.html",
    "href": "posts/13주차과제.html",
    "title": "1. Imports",
    "section": "",
    "text": "!kaggle competitions download -c titanic\n!unzip titanic.zip -d ./titanic\ntrain_data = pd.read_csv('titanic/train.csv')\ntest_data = pd.read_csv('titanic/test.csv')\n!rm titanic.zip\n!rm -rf titanic/\n\nWarning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\nDownloading titanic.zip to /root\n  0%|                                               | 0.00/34.1k [00:00&lt;?, ?B/s]\n100%|██████████████████████████████████████| 34.1k/34.1k [00:00&lt;00:00, 2.95MB/s]\nArchive:  titanic.zip\n  inflating: ./titanic/gender_submission.csv  \n  inflating: ./titanic/test.csv      \n  inflating: ./titanic/train.csv"
  },
  {
    "objectID": "posts/13주차과제.html#a.-y의-분포-xy의-관계-시각화",
    "href": "posts/13주차과제.html#a.-y의-분포-xy의-관계-시각화",
    "title": "1. Imports",
    "section": "A. y의 분포, (X,y)의 관계 시각화",
    "text": "A. y의 분포, (X,y)의 관계 시각화\n\nauto.target_analysis(\n    train_data=df_train,\n    label='SalePrice',\n    fit_distributions=False\n)\n\nTarget variable analysis\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\ndtypes\nunique\nmissing_count\nmissing_ratio\nraw_type\nspecial_types\n\n\n\n\nSalePrice\n1460\n180921.19589\n79442.502883\n34900.0\n129975.0\n163000.0\n214000.0\n755000.0\nint64\n663\n\n\nint\n\n\n\n\n\n\n\n\n\n\n\nTarget variable correlations\n\n\ntrain_data - spearman correlation matrix; focus: absolute correlation for SalePrice &gt;= 0.5\n\n\n\n\n\nFeature interaction between OverallQual/SalePrice in train_data\n\n\n\n\n\nFeature interaction between GrLivArea/SalePrice in train_data\n\n\n\n\n\nFeature interaction between GarageCars/SalePrice in train_data\n\n\n\n\n\nFeature interaction between YearBuilt/SalePrice in train_data\n\n\n\n\n\nFeature interaction between GarageArea/SalePrice in train_data\n\n\n\n\n\nFeature interaction between FullBath/SalePrice in train_data\n\n\n\n\n\nFeature interaction between TotalBsmtSF/SalePrice in train_data\n\n\n\n\n\nFeature interaction between GarageYrBlt/SalePrice in train_data\n\n\n\n\n\nFeature interaction between 1stFlrSF/SalePrice in train_data\n\n\n\n\n\nFeature interaction between YearRemodAdd/SalePrice in train_data\n\n\n\n\n\nFeature interaction between TotRmsAbvGrd/SalePrice in train_data\n\n\n\n\n\nFeature interaction between Fireplaces/SalePrice in train_data\n\n\n\n\n\nFeature interaction between KitchenQual/SalePrice in train_data\n\n\n\n\n\nFeature interaction between ExterQual/SalePrice in train_data"
  },
  {
    "objectID": "posts/13주차과제.html#target-variable-analysis",
    "href": "posts/13주차과제.html#target-variable-analysis",
    "title": "1. Imports",
    "section": "Target variable analysis",
    "text": "Target variable analysis"
  },
  {
    "objectID": "posts/13주차과제.html#b.-중요한-설명변수",
    "href": "posts/13주차과제.html#b.-중요한-설명변수",
    "title": "1. Imports",
    "section": "B. 중요한 설명변수",
    "text": "B. 중요한 설명변수\n\nauto.quick_fit(\n    train_data=df_train,\n    label='SalePrice',\n    show_feature_importance_barplots=True\n)\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20231205_020903/\"\n\n\nModel Prediction for SalePrice\n\n\nUsing validation data for Test points\n\n\n\n\n\nModel Leaderboard\n\n\n\n\n\n\n\n\n\nmodel\nscore_test\nscore_val\npred_time_test\npred_time_val\nfit_time\npred_time_test_marginal\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nLightGBMXT\n-30529.412291\n-32535.182194\n0.014\n0.013309\n2.212614\n0.014\n0.013309\n2.212614\n1\nTrue\n1\n\n\n\n\n\n\n\nFeature Importance for Trained Model\n\n\n\n\n\n\n\n\n\nimportance\nstddev\np_value\nn\np99_high\np99_low\n\n\n\n\nOverallQual\n11716.915326\n705.955990\n1.573763e-06\n5\n13170.488468\n10263.342185\n\n\nGrLivArea\n6071.089919\n394.407430\n2.125470e-06\n5\n6883.180269\n5258.999569\n\n\nGarageCars\n3209.379857\n372.418575\n2.137296e-05\n5\n3976.194851\n2442.564863\n\n\nBsmtFinSF1\n2719.389615\n125.895615\n5.496647e-07\n5\n2978.610426\n2460.168805\n\n\nTotalBsmtSF\n2187.752068\n328.342663\n5.909712e-05\n5\n2863.814149\n1511.689987\n\n\n1stFlrSF\n1688.228494\n248.951746\n5.513520e-05\n5\n2200.823579\n1175.633408\n\n\nNeighborhood\n1534.658374\n377.814804\n4.073284e-04\n5\n2312.584278\n756.732471\n\n\nHalfBath\n881.236830\n353.675558\n2.542584e-03\n5\n1609.459692\n153.013968\n\n\n2ndFlrSF\n872.631138\n123.229525\n4.647906e-05\n5\n1126.362433\n618.899844\n\n\nFireplaces\n868.173697\n364.469844\n2.989959e-03\n5\n1618.622144\n117.725249\n\n\nExterQual\n829.910645\n401.498301\n4.933027e-03\n5\n1656.601195\n3.220095\n\n\nYearRemodAdd\n675.119616\n166.871475\n4.136303e-04\n5\n1018.710290\n331.528942\n\n\nLotArea\n645.731150\n240.401151\n1.933927e-03\n5\n1140.720443\n150.741856\n\n\nFullBath\n587.941341\n102.476487\n1.064026e-04\n5\n798.941844\n376.940838\n\n\nTotRmsAbvGrd\n455.362588\n146.798615\n1.134317e-03\n5\n757.622965\n153.102211\n\n\nBsmtExposure\n452.574670\n100.266016\n2.711034e-04\n5\n659.023783\n246.125556\n\n\nOverallCond\n415.878748\n101.116481\n3.882599e-04\n5\n624.078980\n207.678516\n\n\nYearBuilt\n328.365122\n49.414644\n5.972756e-05\n5\n430.110557\n226.619687\n\n\nMasVnrArea\n291.987049\n102.297731\n1.546173e-03\n5\n502.619491\n81.354608\n\n\nGarageArea\n266.571657\n194.368541\n1.870635e-02\n5\n666.779168\n-133.635855\n\n\nCentralAir\n252.005310\n43.263826\n1.002687e-04\n5\n341.086127\n162.924494\n\n\nLotFrontage\n234.476025\n232.700363\n4.367093e-02\n5\n713.609289\n-244.657239\n\n\nGarageYrBlt\n179.569936\n41.348994\n3.147824e-04\n5\n264.708087\n94.431785\n\n\nBsmtFullBath\n130.488441\n27.317412\n2.176156e-04\n5\n186.735370\n74.241513\n\n\nMSSubClass\n115.302650\n77.955090\n1.486407e-02\n5\n275.813256\n-45.207957\n\n\nBsmtFinType1\n89.671801\n23.805159\n5.438720e-04\n5\n138.686952\n40.656649\n\n\nKitchenAbvGr\n88.796843\n13.144665\n5.597648e-05\n5\n115.861890\n61.731796\n\n\nOpenPorchSF\n76.599171\n32.768044\n3.198232e-03\n5\n144.069027\n9.129315\n\n\nYrSold\n55.041099\n17.204734\n1.010331e-03\n5\n90.465884\n19.616315\n\n\nMSZoning\n52.239352\n30.345403\n9.156204e-03\n5\n114.720955\n-10.242251\n\n\nBsmtQual\n45.470810\n50.360612\n5.681646e-02\n5\n149.164006\n-58.222385\n\n\nPavedDrive\n27.952965\n11.965434\n3.205450e-03\n5\n52.589960\n3.315970\n\n\nFoundation\n15.764567\n21.136187\n8.534210e-02\n5\n59.284268\n-27.755134\n\n\nWoodDeckSF\n14.490268\n49.028633\n2.724108e-01\n5\n115.440900\n-86.460364\n\n\nBsmtUnfSF\n14.251367\n14.062238\n4.304693e-02\n5\n43.205709\n-14.702976\n\n\nScreenPorch\n10.514393\n6.927148\n1.371401e-02\n5\n24.777486\n-3.748700\n\n\nHeatingQC\n10.365567\n6.499702\n1.172951e-02\n5\n23.748544\n-3.017409\n\n\nId\n9.875743\n25.538459\n2.179917e-01\n5\n62.459783\n-42.708298\n\n\nRoofStyle\n8.089551\n53.663840\n3.765020e-01\n5\n118.584139\n-102.405037\n\n\nLandSlope\n6.869875\n7.000689\n4.662255e-02\n5\n21.284391\n-7.544641\n\n\nBsmtFinSF2\n6.856908\n2.939085\n3.220858e-03\n5\n12.908525\n0.805292\n\n\nExterior1st\n4.492434\n8.697021\n1.561884e-01\n5\n22.399720\n-13.414852\n\n\nHouseStyle\n3.571663\n3.437669\n4.042508e-02\n5\n10.649870\n-3.506545\n\n\nBsmtHalfBath\n2.760108\n1.020890\n1.888221e-03\n5\n4.862135\n0.658082\n\n\nMoSold\n1.776995\n18.530416\n4.203498e-01\n5\n39.931378\n-36.377389\n\n\nFireplaceQu\n0.246895\n8.880015\n4.767049e-01\n5\n18.530969\n-18.037180\n\n\nGarageQual\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nGarageCond\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nUtilities\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nLandContour\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\n3SsnPorch\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nPoolArea\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nAlley\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nPoolQC\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nFence\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nMiscFeature\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nMiscVal\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nStreet\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nSaleType\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nLotConfig\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nCondition1\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nSaleCondition\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nMasVnrType\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nGarageType\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nCondition2\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nFunctional\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nBldgType\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nBsmtFinType2\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nBsmtCond\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nKitchenQual\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nHeating\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nRoofMatl\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nLowQualFinSF\n0.000000\n0.000000\n5.000000e-01\n5\n0.000000\n0.000000\n\n\nElectrical\n-0.249231\n0.232446\n9.627220e-01\n5\n0.229380\n-0.727841\n\n\nBedroomAbvGr\n-0.281759\n5.209289\n5.452164e-01\n5\n10.444239\n-11.007758\n\n\nEnclosedPorch\n-0.726761\n1.153234\n8.842100e-01\n5\n1.647763\n-3.101286\n\n\nExterior2nd\n-1.300222\n2.040234\n8.863621e-01\n5\n2.900648\n-5.501092\n\n\nGarageFinish\n-4.409206\n10.141071\n8.070016e-01\n5\n16.471400\n-25.289812\n\n\nExterCond\n-4.973063\n0.837647\n9.999070e-01\n5\n-3.248336\n-6.697791\n\n\nLotShape\n-56.043807\n64.453373\n9.381189e-01\n5\n76.666578\n-188.754193\n\n\n\n\n\n\n\n\n\n\nRows with the highest prediction error\n\n\nRows in this category worth inspecting for the causes of the error\n\n\n\n\n\n\n\n\n\nId\nMSSubClass\nMSZoning\nLotFrontage\nLotArea\nStreet\nAlley\nLotShape\nLandContour\nUtilities\n...\nFence\nMiscFeature\nMiscVal\nMoSold\nYrSold\nSaleType\nSaleCondition\nSalePrice\nSalePrice_pred\nerror\n\n\n\n\n1182\n1183\n60\nRL\n160.0\n15623\nPave\nNaN\nIR1\nLvl\nAllPub\n...\nMnPrv\nNaN\n0\n7\n2007\nWD\nAbnorml\n745000\n458492.125000\n286507.875000\n\n\n1298\n1299\n60\nRL\n313.0\n63887\nPave\nNaN\nIR3\nBnk\nAllPub\n...\nNaN\nNaN\n0\n1\n2008\nNew\nPartial\n160000\n394141.625000\n234141.625000\n\n\n688\n689\n20\nRL\n60.0\n8089\nPave\nNaN\nReg\nHLS\nAllPub\n...\nNaN\nNaN\n0\n10\n2007\nNew\nPartial\n392000\n236151.000000\n155849.000000\n\n\n898\n899\n20\nRL\n100.0\n12919\nPave\nNaN\nIR1\nLvl\nAllPub\n...\nNaN\nNaN\n0\n3\n2010\nNew\nPartial\n611657\n463175.906250\n148481.093750\n\n\n440\n441\n20\nRL\n105.0\n15431\nPave\nNaN\nReg\nLvl\nAllPub\n...\nNaN\nNaN\n0\n4\n2009\nWD\nNormal\n555000\n419354.593750\n135645.406250\n\n\n581\n582\n20\nRL\n98.0\n12704\nPave\nNaN\nReg\nLvl\nAllPub\n...\nNaN\nNaN\n0\n8\n2009\nNew\nPartial\n253293\n382104.968750\n128811.968750\n\n\n769\n770\n60\nRL\n47.0\n53504\nPave\nNaN\nIR2\nHLS\nAllPub\n...\nNaN\nNaN\n0\n6\n2010\nWD\nNormal\n538000\n429698.281250\n108301.718750\n\n\n632\n633\n20\nRL\n85.0\n11900\nPave\nNaN\nReg\nLvl\nAllPub\n...\nNaN\nNaN\n0\n4\n2009\nWD\nFamily\n82500\n182936.734375\n100436.734375\n\n\n4\n5\n60\nRL\n84.0\n14260\nPave\nNaN\nIR1\nLvl\nAllPub\n...\nNaN\nNaN\n0\n12\n2008\nWD\nNormal\n250000\n333203.062500\n83203.062500\n\n\n666\n667\n60\nRL\nNaN\n18450\nPave\nNaN\nIR1\nLvl\nAllPub\n...\nNaN\nNaN\n0\n8\n2007\nWD\nAbnorml\n129000\n210298.609375\n81298.609375\n\n\n\n\n10 rows × 83 columns"
  },
  {
    "objectID": "posts/13주차과제.html#c.-관측치별-해석",
    "href": "posts/13주차과제.html#c.-관측치별-해석",
    "title": "1. Imports",
    "section": "C. 관측치별 해석",
    "text": "C. 관측치별 해석\n\ndf_train.iloc[[1]]\n\n\n\n\n\n\n\n\nId\nMSSubClass\nMSZoning\nLotFrontage\nLotArea\nStreet\nAlley\nLotShape\nLandContour\nUtilities\n...\nPoolArea\nPoolQC\nFence\nMiscFeature\nMiscVal\nMoSold\nYrSold\nSaleType\nSaleCondition\nSalePrice\n\n\n\n\n1\n2\n20\nRL\n80.0\n9600\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2007\nWD\nNormal\n181500\n\n\n\n\n1 rows × 81 columns\n\n\n\n\nauto.explain_rows(\n    train_data=df_train,\n    model=predictr,\n    rows=df_train.iloc[[1]],\n    display_rows=True,\n    plot='waterfall'\n)\n\n\n\n\n\n\n\n\nId\nMSSubClass\nMSZoning\nLotFrontage\nLotArea\nStreet\nAlley\nLotShape\nLandContour\nUtilities\n...\nPoolArea\nPoolQC\nFence\nMiscFeature\nMiscVal\nMoSold\nYrSold\nSaleType\nSaleCondition\nSalePrice\n\n\n\n\n1\n2\n20\nRL\n80.0\n9600\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2007\nWD\nNormal\n181500\n\n\n\n\n1 rows × 81 columns"
  },
  {
    "objectID": "posts/시계열분석발표.html",
    "href": "posts/시계열분석발표.html",
    "title": "시계열분석 발표",
    "section": "",
    "text": "2020.10.08 ~ 2021.12.31 대한민국 누적 코로나 확진자 수\n2021.11월 이후 누적 확진자 수가 대폭 증가하는 경향. (델타 + 오미크론 영향)\n\n\noptions(repr.plot.width = 12, repr.plot.height = 8)\ndata  = read.csv(\"COVID.csv\")\nz = data$n;head(z)\n\n\n624167547475\n\n\n\nforecast::tsdisplay(z, lag.max = 28, main = \"COVID-19 time series\")\n\n\n\n\n\n시도표 상 이분산성이 보임. 또한 ACF 그림을 보아 비정상성 시계열 및 주기가 7인 계절성분이 보이는 것 같음.\n주기가 7인 이유: 같은 주 기준 평일 월, 화에 높고 금,토,일 주말에는 낮은 경향을 보였음.\n따라서 분산 안정화, 차분, 계절 차분할 예정.\n\n\nforecast::BoxCox.lambda(z)\n\n0.0970274555064468\n\n\n\nboxcox_z = forecast::BoxCox(z,lambda= forecast::BoxCox.lambda(z));head(boxcox_z)\n\n\n5.075771842621844.470754453392085.191963310048734.870959706870225.342118683445515.36251256483527\n\n\n\nforecast::tsdisplay(boxcox_z, lag.max = 28, main = \"Boxcox data\")\n\n\n\n\n\n# 단위근검정 H0 : phi=1\nfUnitRoots::adfTest(boxcox_z, lags = 1, type = \"ct\")\nfUnitRoots::adfTest(boxcox_z, lags = 2, type = \"ct\")\n\n\nTitle:\n Augmented Dickey-Fuller Test\n\nTest Results:\n  PARAMETER:\n    Lag Order: 1\n  STATISTIC:\n    Dickey-Fuller: -3.8528\n  P VALUE:\n    0.01646 \n\nDescription:\n Tue Dec  5 15:03:31 2023 by user: \n\n\n\nTitle:\n Augmented Dickey-Fuller Test\n\nTest Results:\n  PARAMETER:\n    Lag Order: 2\n  STATISTIC:\n    Dickey-Fuller: -2.8881\n  P VALUE:\n    0.2023 \n\nDescription:\n Tue Dec  5 15:03:31 2023 by user: \n\n\n\nd_boxcox_z = diff(boxcox_z)\nforecast::tsdisplay(d_boxcox_z, lag.max = 28, main = \"differencing data\")\n\n\n\n\n\nds_d_boxcox_z = diff(d_boxcox_z, 7)\n\nforecast::tsdisplay(ds_d_boxcox_z, lag.max = 28, main = \"seasonal differencing data\")\n\n\n\n\n\n비계절형 ACF에서 1,2 시차에서 절단/ PACF에서는 1,2,3,4? 에서 절단\n비계절형에서는 차수가 더 낮은 ARIMA(0,0,2)모형 선택\n계절형 ACF 7에서 절단, PACF는 7,14,21에서 절단\n계절형에서는 SARIMA(0,0,1)7\n따라서 ds_d_boxcox_z = ARIMA(0,0,2)(0,0,1)7 or boxcox_z = ARIMA(0,1,2)(0,1,1)7을 선택"
  },
  {
    "objectID": "posts/시계열분석발표.html#잔차검정",
    "href": "posts/시계열분석발표.html#잔차검정",
    "title": "시계열분석 발표",
    "section": "잔차검정",
    "text": "잔차검정\n\nforecast::checkresiduals(fit2, main = \"residual\")\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(0,1,1)(0,1,1)[7]\nQ* = 16.188, df = 8, p-value = 0.03976\n\nModel df: 2.   Total lags used: 10\n\n\n\n\n\n\n\nt.test(resid(fit2))\n\n\n    One Sample t-test\n\ndata:  resid(fit2)\nt = -0.8674, df = 449, p-value = 0.3862\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.03251286  0.01260113\nsample estimates:\n   mean of x \n-0.009955862 \n\n\n\nBox.test(resid(fit2), lag=1, type = \"Ljung-Box\")\nBox.test(resid(fit2), lag=6, type = \"Ljung-Box\")\nBox.test(resid(fit2), lag=12, type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  resid(fit2)\nX-squared = 0.075745, df = 1, p-value = 0.7831\n\n\n\n    Box-Ljung test\n\ndata:  resid(fit2)\nX-squared = 7.8473, df = 6, p-value = 0.2495\n\n\n\n    Box-Ljung test\n\ndata:  resid(fit2)\nX-squared = 16.376, df = 12, p-value = 0.1746\n\n\n\nastsa::sarima(boxcox_z, 0,1,1,0,1,1,7, details = T)\n\ninitial  value -1.061301 \niter   2 value -1.311697\niter   3 value -1.330154\niter   4 value -1.338688\niter   5 value -1.344267\niter   6 value -1.345799\niter   7 value -1.345936\niter   8 value -1.346035\niter   9 value -1.346035\niter   9 value -1.346035\niter   9 value -1.346035\nfinal  value -1.346035 \nconverged\ninitial  value -1.371426 \niter   2 value -1.391006\niter   3 value -1.391074\niter   4 value -1.391081\niter   5 value -1.391086\niter   6 value -1.391087\niter   6 value -1.391087\nfinal  value -1.391087 \nconverged\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ma1     sma1\n      -0.3735  -0.8947\ns.e.   0.0436   0.0269\n\nsigma^2 estimated as 0.06032:  log likelihood = -12.31,  aic = 30.62\n\n$degrees_of_freedom\n[1] 440\n\n$ttable\n     Estimate     SE  t.value p.value\nma1   -0.3735 0.0436  -8.5592       0\nsma1  -0.8947 0.0269 -33.3190       0\n\n$AIC\n[1] 0.06927794\n\n$AICc\n[1] 0.06933979\n\n$BIC\n[1] 0.09704701\n\n\n\n\n\n\n여러가지 모형 적합\n\nsummary(fit2)$aic\nsummary(fit2)$sigma2\n\n30.6208505601209\n\n\n0.0603230451585682\n\n\n\nfit3 = arima(boxcox_z, order = c(1,1,0),\n             seasonal = list(order = c(0,1,1), period=7))\nsummary(fit3)\nlmtest::coeftest(fit3)\nsummary(fit3)$aic\nsummary(fit3)$sigma2\n\n\nCall:\narima(x = boxcox_z, order = c(1, 1, 0), seasonal = list(order = c(0, 1, 1), \n    period = 7))\n\nCoefficients:\n          ar1     sma1\n      -0.3002  -0.8918\ns.e.   0.0457   0.0262\n\nsigma^2 estimated as 0.06213:  log likelihood = -18.73,  aic = 43.46\n\nTraining set error measures:\n                       ME      RMSE       MAE        MPE     MAPE      MASE\nTraining set -0.008447679 0.2470426 0.1771224 -0.1556136 2.103485 0.6797119\n                    ACF1\nTraining set -0.05502391\n\n\n\nz test of coefficients:\n\n      Estimate Std. Error  z value  Pr(&gt;|z|)    \nar1  -0.300167   0.045672  -6.5723 4.955e-11 ***\nsma1 -0.891807   0.026243 -33.9828 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n\n43.4628750492228\n\n\n0.0621343484023975\n\n\n\nfit4 = arima(boxcox_z, order = c(2,1,0),\n             seasonal = list(order = c(0,1,1), period=7))\nsummary(fit4)\nlmtest::coeftest(fit4)\nsummary(fit4)$aic\nsummary(fit4)$sigma2\n\n\nCall:\narima(x = boxcox_z, order = c(2, 1, 0), seasonal = list(order = c(0, 1, 1), \n    period = 7))\n\nCoefficients:\n          ar1      ar2     sma1\n      -0.3526  -0.1881  -0.9008\ns.e.   0.0469   0.0475   0.0260\n\nsigma^2 estimated as 0.05991:  log likelihood = -11.01,  aic = 30.02\n\nTraining set error measures:\n                       ME      RMSE     MAE        MPE   MAPE      MASE\nTraining set -0.009928693 0.2425888 0.17824 -0.1738818 2.1011 0.6840005\n                     ACF1\nTraining set -0.004152125\n\n\n\nz test of coefficients:\n\n      Estimate Std. Error  z value  Pr(&gt;|z|)    \nar1  -0.352623   0.046869  -7.5236 5.329e-14 ***\nar2  -0.188129   0.047501  -3.9606 7.478e-05 ***\nsma1 -0.900764   0.025962 -34.6959 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n\n30.0191468250517\n\n\n0.0599141307778544\n\n\n\nfit5 = arima(boxcox_z, order = c(0,1,1),\n             seasonal = list(order = c(0,1,2), period=7))\nsummary(fit5)\nlmtest::coeftest(fit5)\nsummary(fit5)$aic\nsummary(fit5)$sigma2\n\n\nCall:\narima(x = boxcox_z, order = c(0, 1, 1), seasonal = list(order = c(0, 1, 2), \n    period = 7))\n\nCoefficients:\n          ma1     sma1     sma2\n      -0.3817  -0.8546  -0.0462\ns.e.   0.0445   0.0489   0.0482\n\nsigma^2 estimated as 0.06018:  log likelihood = -11.85,  aic = 31.7\n\nTraining set error measures:\n                      ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set -0.01039578 0.243128 0.177735 -0.1806921 2.094994 0.6820624\n                   ACF1\nTraining set 0.01583387\n\n\n\nz test of coefficients:\n\n      Estimate Std. Error  z value Pr(&gt;|z|)    \nma1  -0.381724   0.044519  -8.5745   &lt;2e-16 ***\nsma1 -0.854601   0.048855 -17.4926   &lt;2e-16 ***\nsma2 -0.046235   0.048168  -0.9599   0.3371    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n\n31.700355026282\n\n\n0.0601807670536254\n\n\n\nfit6 = arima(boxcox_z, order = c(1,1,1),\n             seasonal = list(order = c(1,1,1), period=7))\nsummary(fit6)\nlmtest::coeftest(fit6)\nsummary(fit6)$aic\nsummary(fit6)$sigma2\n\n\nCall:\narima(x = boxcox_z, order = c(1, 1, 1), seasonal = list(order = c(1, 1, 1), \n    period = 7))\n\nCoefficients:\n         ar1      ma1    sar1     sma1\n      0.0677  -0.4362  0.0600  -0.9088\ns.e.  0.1010   0.0878  0.0562   0.0265\n\nsigma^2 estimated as 0.0601:  log likelihood = -11.59,  aic = 33.18\n\nTraining set error measures:\n                      ME      RMSE       MAE        MPE     MAPE      MASE\nTraining set -0.01059982 0.2429618 0.1776908 -0.1828248 2.093254 0.6818928\n                    ACF1\nTraining set 0.003942798\n\n\n\nz test of coefficients:\n\n      Estimate Std. Error  z value  Pr(&gt;|z|)    \nar1   0.067739   0.100999   0.6707    0.5024    \nma1  -0.436222   0.087826  -4.9669 6.805e-07 ***\nsar1  0.060033   0.056197   1.0683    0.2854    \nsma1 -0.908787   0.026496 -34.2984 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n\n33.1767403574291\n\n\n0.0600985463564696\n\n\n\nfit7 = forecast::auto.arima(ts(boxcox_z, frequency=7),\n                     test = \"adf\",\n                     seasonal = TRUE, trace = F)\nsummary(fit7)\nlmtest::coeftest(fit7)\nsummary(fit7)$aic\nsummary(fit7)$sigma2\n\nSeries: ts(boxcox_z, frequency = 7) \nARIMA(2,0,2)(2,1,2)[7] with drift \n\nCoefficients:\n         ar1     ar2     ma1      ma2    sar1    sar2     sma1    sma2   drift\n      0.2555  0.7227  0.3816  -0.3275  0.2010  0.0436  -1.0546  0.1210  0.0178\ns.e.  0.3999  0.3942  0.3883   0.1190  0.6152  0.0713   0.6145  0.5625  0.0073\n\nsigma^2 = 0.06043:  log likelihood = -7.96\nAIC=35.93   AICc=36.44   BIC=76.86\n\nTraining set error measures:\n                        ME      RMSE     MAE         MPE     MAPE      MASE\nTraining set -0.0006914901 0.2414096 0.17754 -0.04840449 2.089976 0.4767194\n                     ACF1\nTraining set -0.008133506\n\n\n\nz test of coefficients:\n\n        Estimate Std. Error z value Pr(&gt;|z|)   \nar1    0.2554734  0.3999393  0.6388 0.522966   \nar2    0.7226721  0.3942309  1.8331 0.066785 . \nma1    0.3815957  0.3882700  0.9828 0.325701   \nma2   -0.3274742  0.1190479 -2.7508 0.005945 **\nsar1   0.2009889  0.6152236  0.3267 0.743901   \nsar2   0.0436016  0.0713448  0.6111 0.541107   \nsma1  -1.0546226  0.6144969 -1.7162 0.086119 . \nsma2   0.1210424  0.5625448  0.2152 0.829635   \ndrift  0.0177889  0.0073206  2.4300 0.015100 * \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n\n35.9277358952237\n\n\n0.0604270946682531\n\n\n\nAIC, Sigma^2 가장 작은 ARIMA(2,1,0)(0,1,1)7 적합\n\n\n\n예측\n\nfore_fit = forecast::forecast(fit2, 31)\n\nforecast_z = forecast::InvBoxCox(fore_fit$mean, lambda=forecast::BoxCox.lambda(z))\n\nforecast_z_upper95 = forecast::InvBoxCox(fore_fit$upper[,2], lambda=forecast::BoxCox.lambda(z))\nforecast_z_lower95 = forecast::InvBoxCox(fore_fit$lower[,2], lambda=forecast::BoxCox.lambda(z))\n\n\nplot(1:(length(z)+31), c(z, forecast_z_upper95), type='n', xlab='t', ylab='z', main = \"Prediction for 1 month ahead\")\nlines(1:length(z),z)\nlines((length(z)+1) : (length(z)+31), forecast_z, col='red')\nlines((length(z)+1) : (length(z)+31), forecast::InvBoxCox(fore_fit$lower[,2], lambda=forecast::BoxCox.lambda(z)), col='blue')\nlines((length(z)+1) : (length(z)+31), forecast::InvBoxCox(fore_fit$upper[,2], lambda=forecast::BoxCox.lambda(z)), col='blue')"
  },
  {
    "objectID": "posts/고급딥러닝 최종과제.html",
    "href": "posts/고급딥러닝 최종과제.html",
    "title": "1. 문제 정의",
    "section": "",
    "text": "---\ntitle: \"고급딥러닝 과제\"\nauthor: \"정초윤, 강민구\"\ndate: \"12/16/2023\"\n---\n\n2023-12-16 00:20:44.534294: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2023-12-16 00:20:44.534366: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n## 설치\n## https://www.tensorflow.org/install/pip?hl=ko\n\n## https://www.tensorflow.org/tutorials/keras/regression?hl=ko ##\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport pathlib\nimport seaborn as sns\n\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\nfrom autogluon.tabular import TabularPredictor\nimport autogluon.eda.auto as auto\n\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nimport warnings\nwarnings.filterwarnings('ignore')\n# 현재 사용 가능한 GPU 목록 출력\nprint(\"Available GPUs:\", tf.config.list_physical_devices('GPU'))\n\n# 모든 GPU 사용 방지를 위해 환경 변수 설정\ntf.config.set_visible_devices([], 'GPU')\n\n# GPU 메모리 즉시 할당 방지\nfor gpu in tf.config.list_physical_devices('GPU'):\n    tf.config.experimental.set_memory_growth(gpu, True)\n\n# CPU 사용 설정\ntf.config.set_visible_devices(tf.config.list_physical_devices('CPU'), 'CPU')\nprint(\"Devices:\", tf.config.list_logical_devices())\n\nAvailable GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\nDevices: [LogicalDevice(name='/device:CPU:0', device_type='CPU')]"
  },
  {
    "objectID": "posts/고급딥러닝 최종과제.html#분석",
    "href": "posts/고급딥러닝 최종과제.html#분석",
    "title": "1. 문제 정의",
    "section": "3. 분석",
    "text": "3. 분석\n\n1) Autogluon 모델 적합\n\ntrain_df_DM_YES = df2[df2['isDM'] == 1]\n\ntrain_df_DM_NO = df2[df2['isDM'] == 0].sample(n= len(train_df_DM_YES))\n\ntrain_df_ML = pd.concat([train_df_DM_YES, train_df_DM_NO])\n\n\ntrain_ML = train_df_ML.sample(frac=0.8,random_state=202350543)\n\ntest_ML = train_df_ML.drop(train_ML.index)\n\n\npredictr = TabularPredictor(label = 'isDM',verbosity=False)\n\npredictr.fit(train_ML)\n\nyhat = predictr.predict(train_ML)\n\n\npredictr.leaderboard(train_ML,silent=True)\n\n\n\n\n\n\n\n\nmodel\nscore_test\nscore_val\neval_metric\npred_time_test\npred_time_val\nfit_time\npred_time_test_marginal\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nRandomForestEntr\n0.993165\n0.7484\naccuracy\n0.587886\n0.090811\n2.829365\n0.587886\n0.090811\n2.829365\n1\nTrue\n6\n\n\n1\nRandomForestGini\n0.991709\n0.7424\naccuracy\n0.697779\n0.092154\n2.693105\n0.697779\n0.092154\n2.693105\n1\nTrue\n5\n\n\n2\nKNeighborsDist\n0.982070\n0.7008\naccuracy\n0.280819\n0.014789\n0.075950\n0.280819\n0.014789\n0.075950\n1\nTrue\n2\n\n\n3\nExtraTreesEntr\n0.970530\n0.7484\naccuracy\n0.564734\n0.091941\n1.375337\n0.564734\n0.091941\n1.375337\n1\nTrue\n9\n\n\n4\nExtraTreesGini\n0.962207\n0.7500\naccuracy\n0.669087\n0.084002\n1.505633\n0.669087\n0.084002\n1.505633\n1\nTrue\n8\n\n\n5\nWeightedEnsemble_L2\n0.813066\n0.7880\naccuracy\n0.877253\n0.121386\n8.645233\n0.003401\n0.004898\n1.172318\n2\nTrue\n14\n\n\n6\nXGBoost\n0.803612\n0.7868\naccuracy\n0.197074\n0.015660\n1.072669\n0.197074\n0.015660\n1.072669\n1\nTrue\n11\n\n\n7\nLightGBM\n0.788921\n0.7820\naccuracy\n0.088892\n0.010017\n3.570881\n0.088892\n0.010017\n3.570881\n1\nTrue\n4\n\n\n8\nCatBoost\n0.786889\n0.7772\naccuracy\n0.146421\n0.011742\n37.665931\n0.146421\n0.011742\n37.665931\n1\nTrue\n7\n\n\n9\nKNeighborsUnif\n0.785433\n0.7012\naccuracy\n0.265632\n0.019118\n0.082021\n0.265632\n0.019118\n0.082021\n1\nTrue\n1\n\n\n10\nLightGBMLarge\n0.779510\n0.7708\naccuracy\n0.021442\n0.014143\n3.459894\n0.021442\n0.014143\n3.459894\n1\nTrue\n13\n\n\n11\nNeuralNetFastAI\n0.755028\n0.7512\naccuracy\n2.149975\n0.089431\n162.516337\n2.149975\n0.089431\n162.516337\n1\nTrue\n10\n\n\n12\nNeuralNetTorch\n0.753953\n0.7544\naccuracy\n1.079953\n0.030018\n134.632117\n1.079953\n0.030018\n134.632117\n1\nTrue\n12\n\n\n13\nLightGBMXT\n0.751986\n0.7460\naccuracy\n0.049004\n0.010261\n2.105681\n0.049004\n0.010261\n2.105681\n1\nTrue\n3\n\n\n\n\n\n\n\n\nbest_model = predictr.get_model_best();best_model\n\n'WeightedEnsemble_L2'\n\n\n\ntrain_ML_score = pd.DataFrame({\n    'accuracy_score' : predictr.evaluate(train_ML)['accuracy'],\n    'precision_score' : predictr.evaluate(train_ML)['precision'],\n    'recall_score' : predictr.evaluate(train_ML)['recall'],\n    'f1_score' : predictr.evaluate(train_ML)['f1']\n},index=[predictr.get_model_best()])\n\n\ntest_ML_score = pd.DataFrame({\n    'accuracy_score' : predictr.evaluate(test_ML)['accuracy'],\n    'precision_score' : predictr.evaluate(test_ML)['precision'],\n    'recall_score' : predictr.evaluate(test_ML)['recall'],\n    'f1_score' : predictr.evaluate(test_ML)['f1']\n},index=[predictr.get_model_best()])\n\n\nscores_autogluon = pd.concat([train_ML_score, test_ML_score]);scores_autogluon\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\nWeightedEnsemble_L2\n0.813066\n0.832542\n0.782849\n0.806931\n\n\nWeightedEnsemble_L2\n0.780110\n0.800644\n0.750582\n0.774805\n\n\n\n\n\n\n\n\n\n2) DNN 모델 적합\n\ndf3 = df2.dropna()\n\n\ndf3_dummies = pd.get_dummies(df3[[\"SEX\", \"REGION\", \"INCOME\", \"GLY_CD\", \"OLIG_OCCU_CD\", \"OLIG_PROTE_CD\", \"FAMILIY_DM\", \"SMOKE\" , \"ALCOHOL\", \"RPA\"]])\ndf3_dummies = df3_dummies.astype(int)\ndf3_dummies.head()\n\n\n\n\n\n\n\n\nSEX_Female\nSEX_Male\nREGION_Contry\nREGION_Metro\nINCOME_High\nINCOME_Low\nGLY_CD_1st\nGLY_CD_2nd\nGLY_CD_3rd\nGLY_CD_4rd\n...\nOLIG_PROTE_CD_5rd\nOLIG_PROTE_CD_NO\nFAMILIY_DM_NO\nFAMILIY_DM_YES\nSMOKE_NO\nSMOKE_YES\nALCOHOL_NO\nALCOHOL_YES\nRPA_NO\nRPA_YES\n\n\n\n\n0\n0\n1\n0\n1\n1\n0\n0\n0\n0\n0\n...\n0\n0\n1\n0\n1\n0\n1\n0\n1\n0\n\n\n1\n0\n1\n1\n0\n1\n0\n0\n0\n0\n0\n...\n0\n1\n1\n0\n1\n0\n0\n1\n1\n0\n\n\n2\n0\n1\n1\n0\n0\n1\n0\n0\n0\n0\n...\n0\n1\n1\n0\n0\n1\n0\n1\n0\n1\n\n\n3\n0\n1\n0\n1\n1\n0\n0\n0\n0\n0\n...\n0\n1\n1\n0\n1\n0\n1\n0\n1\n0\n\n\n5\n1\n0\n1\n0\n1\n0\n0\n0\n0\n0\n...\n0\n1\n1\n0\n1\n0\n1\n0\n1\n0\n\n\n\n\n5 rows × 32 columns\n\n\n\n\ndf3_dummies_columns = [\"SEX\", \"REGION\", \"INCOME\", \"GLY_CD\", \"OLIG_OCCU_CD\", \"OLIG_PROTE_CD\", \"FAMILIY_DM\", \"SMOKE\" , \"ALCOHOL\", \"RPA\"]\n\n\ndf3_dummies_not_columns = list(set(df3.columns) - set(df3_dummies_columns));df3_dummies_not_columns\ndf3_not_dummies = df3[df3_dummies_not_columns];df3_not_dummies.head()\n\n\n\n\n\n\n\n\nBMI\nBP_HIGH\nOLIG_PH\nSGPT_ALT\nTOT_CHOLE\nAGE\nHMG\nSGOT_AST\nGAMMA_GTP\nisDM\nBLDS\n\n\n\n\n0\n25.73\n150.0\n5.0\n11.0\n177.0\n81\n12.2\n18.0\n17.0\n1\n112.0\n\n\n1\n22.77\n160.0\n6.0\n20.0\n177.0\n80\n15.7\n26.0\n81.0\n0\n105.0\n\n\n2\n21.56\n170.0\n7.0\n29.0\n217.0\n79\n15.5\n33.0\n45.0\n0\n100.0\n\n\n3\n24.54\n110.0\n6.0\n13.0\n156.0\n80\n13.1\n23.0\n119.0\n0\n76.0\n\n\n5\n25.65\n116.0\n6.0\n16.0\n199.0\n80\n12.1\n22.0\n13.0\n0\n113.0\n\n\n\n\n\n\n\n\ndf4 = pd.concat([df3_dummies, df3_not_dummies], axis = 1);df4.head()\n\n\n\n\n\n\n\n\nSEX_Female\nSEX_Male\nREGION_Contry\nREGION_Metro\nINCOME_High\nINCOME_Low\nGLY_CD_1st\nGLY_CD_2nd\nGLY_CD_3rd\nGLY_CD_4rd\n...\nBP_HIGH\nOLIG_PH\nSGPT_ALT\nTOT_CHOLE\nAGE\nHMG\nSGOT_AST\nGAMMA_GTP\nisDM\nBLDS\n\n\n\n\n0\n0\n1\n0\n1\n1\n0\n0\n0\n0\n0\n...\n150.0\n5.0\n11.0\n177.0\n81\n12.2\n18.0\n17.0\n1\n112.0\n\n\n1\n0\n1\n1\n0\n1\n0\n0\n0\n0\n0\n...\n160.0\n6.0\n20.0\n177.0\n80\n15.7\n26.0\n81.0\n0\n105.0\n\n\n2\n0\n1\n1\n0\n0\n1\n0\n0\n0\n0\n...\n170.0\n7.0\n29.0\n217.0\n79\n15.5\n33.0\n45.0\n0\n100.0\n\n\n3\n0\n1\n0\n1\n1\n0\n0\n0\n0\n0\n...\n110.0\n6.0\n13.0\n156.0\n80\n13.1\n23.0\n119.0\n0\n76.0\n\n\n5\n1\n0\n1\n0\n1\n0\n0\n0\n0\n0\n...\n116.0\n6.0\n16.0\n199.0\n80\n12.1\n22.0\n13.0\n0\n113.0\n\n\n\n\n5 rows × 43 columns\n\n\n\n\ntrain_df_DM_YES = df4[df4['isDM'] == 1]\n\ntrain_df_DM_NO = df4[df4['isDM'] == 0].sample(n= len(train_df_DM_YES))\n\ntrain_df_DNN = pd.concat([train_df_DM_YES, train_df_DM_NO])\n\n\ntrain_DNN = train_df_DNN.sample(frac=0.8,random_state=202350543)\n\ntest_DNN = train_df_DNN.drop(train_DNN.index)\n\ntrain_DNN_stats = train_DNN.describe()\n\ntrain_DNN_stats.pop(\"isDM\")\n\ntrain_DNN_stats = train_DNN_stats.transpose()\n\n\ntrain_labels = train_DNN.pop('isDM')\n\ntest_labels = test_DNN.pop('isDM')\n\ndef norm(x):\n  return (x - train_DNN_stats['mean']) / train_DNN_stats['std']\n\nnormed_train_DNN = norm(train_DNN)\n\nnormed_test_DNN= norm(test_DNN)\n\n\ndef build_model():\n  model = keras.Sequential([\n\n    layers.Dense(128, activation='relu', input_shape=[len(normed_train_DNN.keys())]),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(32, activation='relu'),\n    layers.Dense(1, activation='sigmoid')])\n\n  optimizer = tf.keras.optimizers.Adam(0.0001)\n  model.compile(loss='binary_crossentropy',  # 분류 문제에 적합한 손실 함수 선택\n                  optimizer=optimizer,\n                  metrics=['accuracy'])\n\n  return model\n\n\nmodel = build_model()\nmodel.summary()\n\nclass PrintDot(keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs):\n    if epoch % 100 == 0: print('')\n    print('.', end='')\n\nearly_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n\nEPOCHS = 500\nhistory = model.fit(\n  normed_train_DNN, train_labels,\n  epochs=EPOCHS, validation_split = 0.2, verbose=0,\n  callbacks=[early_stop, PrintDot()])\n# callbacks=[early_stopping],\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense (Dense)               (None, 128)               5504      \n                                                                 \n dense_1 (Dense)             (None, 64)                8256      \n                                                                 \n dense_2 (Dense)             (None, 32)                2080      \n                                                                 \n dense_3 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 15873 (62.00 KB)\nTrainable params: 15873 (62.00 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n.................................\n\n\n\npd.DataFrame(history.history)\n\n\n\n\n\n\n\n\nloss\naccuracy\nval_loss\nval_accuracy\n\n\n\n\n0\n0.556735\n0.711561\n0.531616\n0.730941\n\n\n1\n0.517718\n0.740870\n0.522321\n0.737209\n\n\n2\n0.511950\n0.744356\n0.518307\n0.740279\n\n\n3\n0.508225\n0.746082\n0.517569\n0.738360\n\n\n4\n0.505757\n0.747282\n0.516406\n0.740343\n\n\n5\n0.503538\n0.747474\n0.515869\n0.738808\n\n\n6\n0.501774\n0.749073\n0.515059\n0.738808\n\n\n7\n0.500189\n0.750704\n0.514988\n0.738232\n\n\n8\n0.498560\n0.752462\n0.513398\n0.741814\n\n\n9\n0.497225\n0.752159\n0.513124\n0.740215\n\n\n10\n0.495642\n0.751791\n0.513934\n0.740471\n\n\n11\n0.494188\n0.753006\n0.514217\n0.740918\n\n\n12\n0.492734\n0.753646\n0.512184\n0.740343\n\n\n13\n0.491608\n0.753486\n0.511012\n0.741686\n\n\n14\n0.490308\n0.755564\n0.512571\n0.741366\n\n\n15\n0.489122\n0.754733\n0.510942\n0.743093\n\n\n16\n0.487775\n0.755724\n0.511501\n0.742006\n\n\n17\n0.486459\n0.755245\n0.511314\n0.740471\n\n\n18\n0.485243\n0.757307\n0.510339\n0.741750\n\n\n19\n0.484247\n0.757787\n0.511297\n0.742581\n\n\n20\n0.483101\n0.758315\n0.512405\n0.741878\n\n\n21\n0.482075\n0.757995\n0.511575\n0.743477\n\n\n22\n0.481025\n0.759242\n0.510195\n0.743157\n\n\n23\n0.480333\n0.759722\n0.511163\n0.742709\n\n\n24\n0.479156\n0.760409\n0.510457\n0.742901\n\n\n25\n0.478207\n0.760074\n0.512175\n0.742006\n\n\n26\n0.477491\n0.761433\n0.511607\n0.742773\n\n\n27\n0.476682\n0.762120\n0.512665\n0.741110\n\n\n28\n0.475686\n0.762824\n0.518998\n0.736506\n\n\n29\n0.474985\n0.762408\n0.513225\n0.741686\n\n\n30\n0.474104\n0.763575\n0.512610\n0.741238\n\n\n31\n0.473453\n0.764215\n0.514955\n0.740407\n\n\n32\n0.472442\n0.764343\n0.512826\n0.742134\n\n\n\n\n\n\n\n\ndef plot_history(history):\n  hist = pd.DataFrame(history.history)\n  hist['epoch'] = history.epoch\n\n  plt.figure(figsize=(8,12))\n\n  plt.subplot(2,1,1)\n  plt.xlabel('Epoch')\n  plt.ylabel('Loss')\n  plt.plot(hist['epoch'], hist['loss'], label = 'Train loss')\n    \n  plt.plot(hist['epoch'], hist['val_loss'], label = 'Validation loss')\n  plt.title('Loss')\n  plt.legend()\n \n    \n  plt.subplot(2,1,2)\n  plt.xlabel('Epoch')\n  plt.ylabel('Accuracy')\n  plt.plot(hist['epoch'], hist['accuracy'], label = 'Train accuracy')\n\n  plt.plot(hist['epoch'],hist ['val_accuracy'], label = 'Validation val_accuracy')\n\n  plt.title('Accuracy')\n  plt.legend()\n  plt.show()\n\nplot_history(history)\n\n\n\n\n- train, test 셋 accuracy, loss\n\nmodel.evaluate(normed_train_DNN, train_labels, verbose=2)\n\n2443/2443 - 1s - loss: 0.4775 - accuracy: 0.7619 - 1s/epoch - 527us/step\n\n\n[0.47747179865837097, 0.7618962526321411]\n\n\n\nmodel.evaluate(normed_test_DNN, test_labels, verbose=2)\n\n611/611 - 0s - loss: 0.5174 - accuracy: 0.7367 - 348ms/epoch - 570us/step\n\n\n[0.5173986554145813, 0.7367478609085083]\n\n\n\nthreshold = 0.5\npred_y = np.where(model.predict(normed_test_DNN) &gt; threshold, 1, 0)\n\n611/611 [==============================] - 0s 585us/step\n\n\n\ny_hat = pred_y.reshape(-1)\ny = test_labels.values"
  },
  {
    "objectID": "posts/고급딥러닝 최종과제.html#dnn-vs-autogluon",
    "href": "posts/고급딥러닝 최종과제.html#dnn-vs-autogluon",
    "title": "1. 문제 정의",
    "section": "4. DNN VS AUTOGLUON",
    "text": "4. DNN VS AUTOGLUON\n\nmetrics = [accuracy_score,\n           precision_score,\n           recall_score,\n           f1_score]\n\ntest_DNN_score = pd.DataFrame({m.__name__:[m(y,y_hat).round(6)] for m in metrics},index=['DNN'])\n\n\nscores = pd.concat([test_DNN_score, test_ML_score]);scores\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\nDNN\n0.736748\n0.760040\n0.696735\n0.727012\n\n\nWeightedEnsemble_L2\n0.780110\n0.800644\n0.750582\n0.774805"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "test1",
    "section": "",
    "text": ":::{#quarto-listing-pipeline .hidden} \\(e = mC^2\\)\n:::{.hidden render-id=“pipeline-listing-listing”}\n:::{.list .quarto-listing-default}\n\n\n  \n\n\n\n\n시계열분석 발표\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n문제 정의\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\nImports\n\n\n\n\n\n\n\n\n\n\n \n\n\n:::\n\n\n\nNo matching items\n\n:::\n:::"
  }
]